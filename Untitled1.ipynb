{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85261e1b-0a60-4c13-a195-bc9f033ecf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justinzhao/mambaforge/envs/base38/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torchtext>=0.13.0 is not installed, so the following tokenizers are not available: {'bert'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justinzhao/mambaforge/envs/base38/lib/python3.8/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import whylogs as why\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from ludwig.automl.base_config import get_dataset_info\n",
    "from ludwig.constants import AUDIO, BINARY, CATEGORY, DATE, IMAGE, NUMBER, TEXT\n",
    "from ludwig.datasets import titanic\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae0af87-38b6-43de-a754-2c4eb71f2dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def infer_type(field: FieldInfo, missing_value_percent: float, row_count: int) -> str:\n",
    "#     \"\"\"Perform type inference on field.\n",
    "\n",
    "#     # Inputs\n",
    "#     :param field: (FieldInfo) object describing field\n",
    "#     :param missing_value_percent: (float) percent of missing values in the column\n",
    "#     :param row_count: (int) total number of entries in original dataset\n",
    "\n",
    "#     # Return\n",
    "#     :return: (str) feature type\n",
    "#     \"\"\"\n",
    "#     if field.dtype == DATE:\n",
    "#         return DATE\n",
    "\n",
    "#     num_distinct_values = field.num_distinct_values\n",
    "#     if num_distinct_values == 0:\n",
    "#         return CATEGORY\n",
    "#     distinct_values = field.distinct_values\n",
    "#     if num_distinct_values <= 2 and missing_value_percent == 0:\n",
    "#         # Check that all distinct values are conventional bools.\n",
    "#         if strings_utils.are_conventional_bools(distinct_values):\n",
    "#             return BINARY\n",
    "\n",
    "#     if field.image_values >= 3:\n",
    "#         return IMAGE\n",
    "\n",
    "#     if field.audio_values >= 3:\n",
    "#         return AUDIO\n",
    "\n",
    "#     # Use CATEGORY if:\n",
    "#     # - The number of distinct values is significantly less than the total number of examples.\n",
    "#     # - The distinct values are not all numbers.\n",
    "#     # - The distinct values are all numbers but comprise of a perfectly sequential list of integers that suggests the\n",
    "#     #   values represent categories.\n",
    "#     if num_distinct_values < row_count * CATEGORY_TYPE_DISTINCT_VALUE_PERCENTAGE_CUTOFF and (\n",
    "#         (not strings_utils.are_all_numbers(distinct_values)) or strings_utils.are_sequential_integers(distinct_values)\n",
    "#     ):\n",
    "#         return CATEGORY\n",
    "\n",
    "#     # Use NUMBER if all of the distinct values are numbers.\n",
    "#     if strings_utils.are_all_numbers(distinct_values):\n",
    "#         return NUMBER\n",
    "\n",
    "#     # TODO (ASN): add other modalities (image, etc. )\n",
    "#     # Fallback to TEXT.\n",
    "#     return TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c22d207f-f327-4ca7-a9e7-852423b3c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom resolver.\n",
    "\n",
    "from whylogs.core.resolvers import Resolver\n",
    "from whylogs.core.datatypes import DataType, Fractional, Integral\n",
    "from typing import Dict, List\n",
    "from whylogs.core.metrics import StandardMetric\n",
    "from whylogs.core.metrics.metrics import Metric, OperationResult, MetricConfig\n",
    "from whylogs.core.metrics.metric_components import FractionalComponent\n",
    "from whylogs.core.preprocessing import PreprocessedColumn\n",
    "from typing import Any, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "from whylogs.core.configs import SummaryConfig\n",
    "from whylogs.core.datatypes import DataType, Fractional, Integral, String\n",
    "\n",
    "IMAGE_EXTENSIONS = (\".png\", \".jpg\", \".jpeg\", \".tiff\", \".bmp\", \".gif\")\n",
    "\n",
    "\n",
    "def is_image(src_path: str, img_entry: Union[bytes, str], column: str) -> bool:\n",
    "    if not isinstance(img_entry, str):\n",
    "        return False\n",
    "    try:\n",
    "        import imghdr\n",
    "\n",
    "        path = get_abs_path(src_path, img_entry)\n",
    "        bytes_obj = get_bytes_obj_from_path(path)\n",
    "        if isinstance(bytes_obj, bytes):\n",
    "            return imghdr.what(None, bytes_obj) is not None\n",
    "        return imghdr.what(bytes_obj) is not None\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_image_score(src_path, img_entry, column: str):\n",
    "    \"\"\"Used for AutoML For image inference, want to bias towards both readable images, but also account for\n",
    "    unreadable (i.e. expired) urls with image extensions.\"\"\"\n",
    "    if is_image(src_path, img_entry, column):\n",
    "        return 1\n",
    "    elif isinstance(img_entry, str) and img_entry.lower().endswith(IMAGE_EXTENSIONS):\n",
    "        return 0.5\n",
    "    return 0\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class IsImageMetric(Metric):\n",
    "    score: FractionalComponent\n",
    "    name = \"ludwig_metric\"\n",
    "    \n",
    "    @property\n",
    "    def namespace(self) -> str:\n",
    "        return \"is_image\"\n",
    "\n",
    "    def columnar_update(self, view: PreprocessedColumn) -> OperationResult:\n",
    "        successes = 0\n",
    "        # if view.numpy.len > 0:\n",
    "        #     if view.numpy.ints is not None:\n",
    "        #         self.hll.value.update_np(view.numpy.ints)\n",
    "        #         successes += len(view.numpy.ints)\n",
    "        #     if view.numpy.floats is not None:\n",
    "        #         self.hll.value.update_np(view.numpy.floats)\n",
    "        #         successes += len(view.numpy.floats)\n",
    "        if view.pandas.strings is not None:\n",
    "            # self.hll.value.update_str_list(view.pandas.strings.to_list())\n",
    "            # self.score.value.set(\n",
    "            #     is_image_score(None, view.pandas.strings.to_list()[0], column=\"\"))\n",
    "            # self.score.value = is_image_score(None, view.pandas.strings.to_list()[0], column=\"\")\n",
    "            self.score.set(is_image_score(None, view.pandas.strings.to_list()[0], column=\"\"))\n",
    "            successes += len(view.pandas.strings)\n",
    "\n",
    "        # update everything in the remaining lists\n",
    "        # if view.list.ints:\n",
    "        #     self.hll.value.update_int_list(view.list.ints)\n",
    "        #     successes += len(view.list.ints)\n",
    "        # if view.list.floats:\n",
    "        #     self.hll.value.update_double_list(view.list.floats)\n",
    "        #     successes += len(view.list.floats)\n",
    "        if view.list.strings:\n",
    "            # self.hll.value.update_str_list(view.list.strings)\n",
    "            successes += len(view.list.strings)\n",
    "\n",
    "        failures = 0\n",
    "        if view.list.objs:\n",
    "            failures = len(view.list.objs)\n",
    "        return OperationResult(successes=successes, failures=failures)\n",
    "\n",
    "    def to_summary_dict(self, cfg: SummaryConfig) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"image_score\": self.score.value\n",
    "            # \"est\": self.hll.value.get_estimate(),\n",
    "            # f\"upper_{cfg.hll_stddev}\": self.hll.value.get_upper_bound(cfg.hll_stddev),\n",
    "            # f\"lower_{cfg.hll_stddev}\": self.hll.value.get_lower_bound(cfg.hll_stddev),\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def zero(cls, config: MetricConfig) -> \"IsImageMetric\":\n",
    "        return IsImageMetric(score=FractionalComponent(0.0))\n",
    "\n",
    "\n",
    "class LudwigWhyResolver(Resolver):\n",
    "    \"\"\"Resolver that keeps distribution metrics for Fractional and frequent items for Integral, and counters and types metrics for all data types.\"\"\"\n",
    "\n",
    "    def resolve(self, name: str, why_type: DataType, column_schema) -> Dict[str, Metric]:\n",
    "        metrics: List[StandardMetric] = [StandardMetric.counts, StandardMetric.types]\n",
    "\n",
    "        if isinstance(why_type, Integral):\n",
    "            metrics.append(StandardMetric.distribution)\n",
    "            metrics.append(StandardMetric.ints)\n",
    "            metrics.append(StandardMetric.cardinality)\n",
    "            metrics.append(StandardMetric.frequent_items)\n",
    "        elif isinstance(why_type, Fractional):\n",
    "            metrics.append(StandardMetric.cardinality)\n",
    "            metrics.append(StandardMetric.distribution)\n",
    "        elif isinstance(why_type, String):  # Catch all category as we map 'object' here\n",
    "            # NOTE(justin):\n",
    "            # Could implement image functions as custom metric under String.\n",
    "            # Could implement image functions as custom type with custom metric.\n",
    "            # Need to implement custom metric anyways.\n",
    "            metrics.append(StandardMetric.cardinality)\n",
    "            metrics.append(IsImageMetric)\n",
    "            if column_schema.cfg.track_unicode_ranges:\n",
    "                metrics.append(StandardMetric.unicode_range)\n",
    "            metrics.append(StandardMetric.distribution)  # 'object' columns can contain Decimal\n",
    "            metrics.append(StandardMetric.frequent_items)\n",
    "\n",
    "        if column_schema.cfg.fi_disabled:\n",
    "            metrics.remove(StandardMetric.frequent_items)\n",
    "\n",
    "        result: Dict[str, Metric] = {}\n",
    "        for m in metrics:\n",
    "            result[m.name] = m.zero(column_schema.cfg)\n",
    "        return result\n",
    "    \n",
    "\n",
    "    \n",
    "class ImageType(DataType[int]):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(str)\n",
    "\n",
    "    @classmethod\n",
    "    def _do_match(cls, dtype_or_type: Any, maybe_type: Optional[Any]) -> bool:\n",
    "        if maybe_type:\n",
    "            dtype_or_type = maybe_type  # type: ignore\n",
    "\n",
    "        if not isinstance(dtype_or_type, type):\n",
    "            # Potentially handle numpy arrays?\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "\n",
    "class AudioType(DataType[int]):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(str)\n",
    "\n",
    "    @classmethod\n",
    "    def _do_match(cls, dtype_or_type: Any, maybe_type: Optional[Any]) -> bool:\n",
    "        if maybe_type:\n",
    "            dtype_or_type = maybe_type  # type: ignore\n",
    "\n",
    "        if not isinstance(dtype_or_type, type):\n",
    "            # Potentially handle numpy arrays?\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "\n",
    "from whylogs.core import DatasetSchema\n",
    "\n",
    "\n",
    "class LudwigWhySchema(DatasetSchema):\n",
    "    # resolvers = LudwigWhyResolver()\n",
    "    types: Dict[str, Any] = {}\n",
    "    default_configs: MetricConfig = MetricConfig()\n",
    "    # type_mapper: TypeMapper = StandardTypeMapper(custom_types=[ImageType, AudioType])\n",
    "    # type_mapper: TypeMapper = StandardTypeMapper()\n",
    "    resolvers: Resolver = LudwigWhyResolver()\n",
    "    cache_size: int = 1024\n",
    "    schema_based_automerge: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29aa2899-9b19-43c9-970a-4ed56a1eea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, test_set, _ = titanic.load(split=True)\n",
    "\n",
    "# start = time.time()\n",
    "# get_dataset_info(training_set)\n",
    "# end = time.time()\n",
    "# print(end - start)\n",
    "\n",
    "# Log data with whylogs & create profile\n",
    "results = why.log(pandas=training_set, schema=LudwigWhySchema())\n",
    "\n",
    "# start = time.time()\n",
    "profile = results.profile()\n",
    "prof_view = profile.view()\n",
    "# print(prof_view.to_pandas())\n",
    "\n",
    "# Create dictionary of feature name -> ColumnView\n",
    "column_profiles = {}\n",
    "for feature_name in training_set.keys():\n",
    "    # print(\"---------------------------------------------------------------------------------------------\")\n",
    "    # print(f\"FEATURE NAME: {feature_name}\")\n",
    "    # https://github.com/whylabs/whylogs/blob/d22609ef684805bdbc7cf734eb9265bca0dcb5b9/python/whylogs/core/view/column_profile_view.py#L67-L72\n",
    "    # print(prof_view.get_columns()[feature_name].to_protobuf())\n",
    "    column_profiles[feature_name] = prof_view.get_columns()[feature_name]\n",
    "    # dataset_profile.feature_profiles[feature_name].whylogs_metrics = prof_view.get_columns()[feature_name].to_protobuf()\n",
    "\n",
    "# end = time.time()\n",
    "# print(end - start)\n",
    "\n",
    "# Create profile view dataframe\n",
    "prof_view = profile.view()\n",
    "# print(prof_view.to_pandas())\n",
    "prof_view_pandas = prof_view.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2a1f29b7-6dc0-4ed7-a434-d799c4f7d358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PassengerId' 'Survived' 'Pclass' 'Name' 'Sex' 'Age' 'SibSp' 'Parch'\n",
      " 'Ticket' 'Fare' 'Cabin' 'Embarked']\n",
      "{'cardinality/est': 901.7294570137847,\n",
      " 'cardinality/lower_1': 890.2134308483327,\n",
      " 'cardinality/upper_1': 913.5326015182238,\n",
      " 'counts/n': 891,\n",
      " 'counts/null': 0,\n",
      " 'distribution/max': 891.0,\n",
      " 'distribution/mean': 446.0,\n",
      " 'distribution/median': 449.0,\n",
      " 'distribution/min': 1.0,\n",
      " 'distribution/n': 891,\n",
      " 'distribution/q_01': 14.0,\n",
      " 'distribution/q_05': 50.0,\n",
      " 'distribution/q_10': 94.0,\n",
      " 'distribution/q_25': 226.0,\n",
      " 'distribution/q_75': 669.0,\n",
      " 'distribution/q_90': 802.0,\n",
      " 'distribution/q_95': 847.0,\n",
      " 'distribution/q_99': 883.0,\n",
      " 'distribution/stddev': 257.3538420152301,\n",
      " 'frequent_items/frequent_strings': [],\n",
      " 'ints/max': 891,\n",
      " 'ints/min': 1,\n",
      " 'types/boolean': 0,\n",
      " 'types/fractional': 0,\n",
      " 'types/integral': 891,\n",
      " 'types/object': 0,\n",
      " 'types/string': 0}\n",
      "{'cardinality/est': 3.000000014901161,\n",
      " 'cardinality/lower_1': 3.0,\n",
      " 'cardinality/upper_1': 3.0001498026537594,\n",
      " 'counts/n': 891,\n",
      " 'counts/null': 2,\n",
      " 'distribution/max': nan,\n",
      " 'distribution/mean': 0.0,\n",
      " 'distribution/median': None,\n",
      " 'distribution/min': nan,\n",
      " 'distribution/n': 0,\n",
      " 'distribution/q_01': None,\n",
      " 'distribution/q_05': None,\n",
      " 'distribution/q_10': None,\n",
      " 'distribution/q_25': None,\n",
      " 'distribution/q_75': None,\n",
      " 'distribution/q_90': None,\n",
      " 'distribution/q_95': None,\n",
      " 'distribution/q_99': None,\n",
      " 'distribution/stddev': 0.0,\n",
      " 'frequent_items/frequent_strings': [FrequentItem(value='S', est=644, upper=644, lower=644),\n",
      "                                     FrequentItem(value='C', est=168, upper=168, lower=168),\n",
      "                                     FrequentItem(value='Q', est=77, upper=77, lower=77)],\n",
      " 'ludwig_metric/image_score': 0,\n",
      " 'types/boolean': 0,\n",
      " 'types/fractional': 0,\n",
      " 'types/integral': 0,\n",
      " 'types/object': 0,\n",
      " 'types/string': 889}\n",
      "FrequentItem(value='S', est=644, upper=644, lower=644)\n",
      "FrequentItem(value='C', est=168, upper=168, lower=168)\n",
      "FrequentItem(value='Q', est=77, upper=77, lower=77)\n"
     ]
    }
   ],
   "source": [
    "# prof_view_pandas.loc[[\"PassengerId\"]]\n",
    "# prof_view_pandas.iloc[0]\n",
    "# for x in prof_view_pandas.index.values:\n",
    "#     print(x)\n",
    "\n",
    "# prof_view_pandas.iloc[0][\"counts/n\"]\n",
    "from pprint import pprint\n",
    "print(prof_view_pandas.index.values)\n",
    "pprint(column_profiles['PassengerId'].to_summary_dict())\n",
    "pprint(column_profiles['Embarked'].to_summary_dict())\n",
    "# pprint(column_profiles['Embarked'].get_metric(\"frequent_items\").frequent_strings.value)\n",
    "# pprint(column_profiles['Embarked'].to_summary_dict()[\"frequent_items/frequent_strings\"])\n",
    "max_occurence = column_profiles['Embarked'].to_summary_dict()[\"frequent_items/frequent_strings\"][0].est\n",
    "min_occurence = column_profiles['Embarked'].to_summary_dict()[\"frequent_items/frequent_strings\"][-1].est\n",
    "for frequent_item in column_profiles['Embarked'].to_summary_dict()[\"frequent_items/frequent_strings\"]:\n",
    "    print(frequent_item)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26c511bd-c415-451c-b489-3ae72f3bab22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      3\n",
      "1      1\n",
      "2      3\n",
      "3      1\n",
      "4      3\n",
      "      ..\n",
      "886    2\n",
      "887    1\n",
      "888    3\n",
      "889    1\n",
      "890    3\n",
      "Name: Pclass, Length: 891, dtype: int64\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Lets say we're interested in defining a constraint on the number of \"legs\". From output above we see\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# that there are the following metrics on column \"legs\": [counts, types, distribution, ints, cardinality, frequent_items]\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# lets look at what the distribution metric contains:\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# distribution_values = profile_view.get_column(\"legs\").get_metric(\"distribution\").to_summary_dict()\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# distribution_values\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(training_set[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPclass\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcolumn_profiles\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto_summary_dict())\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature_name, column_profile \u001b[38;5;129;01min\u001b[39;00m column_profiles\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mget_type_from_column_profile(column_profile)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: '2'"
     ]
    }
   ],
   "source": [
    "def get_type_from_column_profile(column_profile):\n",
    "    \"\"\"Returns the Ludwig data type from a whylogs column profile.\"\"\"\n",
    "    column_profile_summary = column_profile.to_summary_dict()\n",
    "\n",
    "    cardinality_est = int(column_profile_summary[\"cardinality/est\"])\n",
    "    counts_n = column_profile_summary[\"counts/n\"]\n",
    "    \n",
    "    if not cardinality_est:\n",
    "        return CATEGORY\n",
    "    \n",
    "    if column_profile_summary[\"types/boolean\"] > counts_n * 0.5:\n",
    "        return BOOLEAN\n",
    "    \n",
    "    if column_profile_summary[\"types/integral\"] > counts_n * 0.5:\n",
    "        if cardinality_est < counts_n * 0.5:\n",
    "            return CATEGORY\n",
    "        return NUMBER\n",
    "    \n",
    "    if column_profile_summary[\"types/fractional\"] > counts_n * 0.5:\n",
    "        return NUMBER\n",
    "    \n",
    "    if cardinality_est < counts_n * 0.5:\n",
    "        return CATEGORY\n",
    "\n",
    "    # Fallback to text.\n",
    "    return TEXT\n",
    "\n",
    "    # Lets say we're interested in defining a constraint on the number of \"legs\". From output above we see\n",
    "    # that there are the following metrics on column \"legs\": [counts, types, distribution, ints, cardinality, frequent_items]\n",
    "    # lets look at what the distribution metric contains:\n",
    "    # distribution_values = profile_view.get_column(\"legs\").get_metric(\"distribution\").to_summary_dict()\n",
    "    # distribution_values\n",
    "\n",
    "print(training_set[\"Pclass\"])\n",
    "print(column_profiles[\"2\"].to_summary_dict())\n",
    "\n",
    "for feature_name, column_profile in column_profiles.items():\n",
    "    print(f\"{feature_name}: {get_type_from_column_profile(column_profile)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6649c9e2-6438-44ae-8a34-19f489a3c4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_features:\n",
    "#     - name: Pclass\n",
    "#       type: category\n",
    "#     - name: Sex\n",
    "#       type: category\n",
    "#     - name: Age\n",
    "#       type: number\n",
    "#       preprocessing:\n",
    "#           missing_value_strategy: fill_with_mean\n",
    "#     - name: SibSp\n",
    "#       type: number\n",
    "#     - name: Parch\n",
    "#       type: number\n",
    "#     - name: Fare\n",
    "#       type: number\n",
    "#       preprocessing:\n",
    "#           missing_value_strategy: fill_with_mean\n",
    "#     - name: Embarked\n",
    "#       type: category\n",
    "\n",
    "# output_features:\n",
    "#     - name: Survived\n",
    "#       type: binary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
