{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d952e8e-2d10-4e68-830f-caca4fe62661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset is:      PassengerId  Survived  Pclass  \\\n",
      "0              1       0.0       3   \n",
      "1              2       1.0       1   \n",
      "2              3       1.0       3   \n",
      "3              4       1.0       1   \n",
      "4              5       0.0       3   \n",
      "..           ...       ...     ...   \n",
      "886          887       0.0       2   \n",
      "887          888       1.0       1   \n",
      "888          889       0.0       3   \n",
      "889          890       1.0       1   \n",
      "890          891       0.0       3   \n",
      "\n",
      "                                                  Name     Sex   Age  SibSp  \\\n",
      "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                             Allen, Mr. William Henry    male  35.0      0   \n",
      "..                                                 ...     ...   ...    ...   \n",
      "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
      "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
      "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
      "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
      "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
      "\n",
      "     Parch            Ticket     Fare Cabin Embarked  \n",
      "0        0         A/5 21171   7.2500   NaN        S  \n",
      "1        0          PC 17599  71.2833   C85        C  \n",
      "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3        0            113803  53.1000  C123        S  \n",
      "4        0            373450   8.0500   NaN        S  \n",
      "..     ...               ...      ...   ...      ...  \n",
      "886      0            211536  13.0000   NaN        S  \n",
      "887      0            112053  30.0000   B42        S  \n",
      "888      2        W./C. 6607  23.4500   NaN        S  \n",
      "889      0            111369  30.0000  C148        C  \n",
      "890      0            370376   7.7500   NaN        Q  \n",
      "\n",
      "[891 rows x 12 columns]\n",
      "\n",
      "╒════════════════════════╕\n",
      "│ EXPERIMENT DESCRIPTION │\n",
      "╘════════════════════════╛\n",
      "\n",
      "╒══════════════════╤════════════════════════════════════════════════════════════════════════════════════════════════╕\n",
      "│ Experiment name  │ simple_experiment                                                                              │\n",
      "├──────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Model name       │ simple_model                                                                                   │\n",
      "├──────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Output directory │ /Users/justinzhao/ludwig/results/simple_experiment_simple_model                                │\n",
      "├──────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ ludwig_version   │ '0.6.dev'                                                                                      │\n",
      "├──────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ command          │ ('/Users/justinzhao/mambaforge/envs/base38/lib/python3.8/site-packages/ipykernel_launcher.py ' │\n",
      "│                  │  '-f '                                                                                         │\n",
      "│                  │  '/Users/justinzhao/Library/Jupyter/runtime/kernel-2d4ac5bb-d4ae-42be-b265-b4408f8513aa.json') │\n",
      "├──────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ commit_hash      │ '6a9569ad1e0c'                                                                                 │\n",
      "├──────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ random_seed      │ 42                                                                                             │\n",
      "├──────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ data_format      │ \"<class 'pandas.core.frame.DataFrame'>\"                                                        │\n",
      "├──────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ torch_version    │ '1.11.0'                                                                                       │\n",
      "├──────────────────┼────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ compute          │ {'num_nodes': 1}                                                                               │\n",
      "╘══════════════════╧════════════════════════════════════════════════════════════════════════════════════════════════╛\n",
      "\n",
      "╒═══════════════╕\n",
      "│ LUDWIG CONFIG │\n",
      "╘═══════════════╛\n",
      "\n",
      "{   'combiner': {   'activation': 'relu',\n",
      "                    'bias_initializer': 'zeros',\n",
      "                    'dropout': 0.0,\n",
      "                    'fc_layers': None,\n",
      "                    'flatten_inputs': False,\n",
      "                    'norm': None,\n",
      "                    'norm_params': None,\n",
      "                    'num_fc_layers': 0,\n",
      "                    'output_size': 256,\n",
      "                    'residual': False,\n",
      "                    'type': 'concat',\n",
      "                    'use_bias': True,\n",
      "                    'weights_initializer': 'xavier_uniform'},\n",
      "    'defaults': {   'audio': {   'preprocessing': {   'audio_file_length_limit_in_s': 7.5,\n",
      "                                                      'in_memory': True,\n",
      "                                                      'missing_value_strategy': 'backfill',\n",
      "                                                      'norm': None,\n",
      "                                                      'num_fft_points': None,\n",
      "                                                      'num_filter_bands': 80,\n",
      "                                                      'padding_value': 0,\n",
      "                                                      'type': 'fbank',\n",
      "                                                      'window_length_in_s': 0.04,\n",
      "                                                      'window_shift_in_s': 0.02,\n",
      "                                                      'window_type': 'hamming'}},\n",
      "                    'bag': {   'preprocessing': {   'fill_value': '<UNK>',\n",
      "                                                    'lowercase': False,\n",
      "                                                    'missing_value_strategy': 'fill_with_const',\n",
      "                                                    'most_common': 10000,\n",
      "                                                    'tokenizer': 'space'}},\n",
      "                    'binary': {   'preprocessing': {   'missing_value_strategy': 'fill_with_false'}},\n",
      "                    'category': {   'preprocessing': {   'fill_value': '<UNK>',\n",
      "                                                         'lowercase': False,\n",
      "                                                         'missing_value_strategy': 'fill_with_const',\n",
      "                                                         'most_common': 10000}},\n",
      "                    'date': {   'preprocessing': {   'datetime_format': None,\n",
      "                                                     'fill_value': '',\n",
      "                                                     'missing_value_strategy': 'fill_with_const'}},\n",
      "                    'h3': {   'preprocessing': {   'fill_value': 576495936675512319,\n",
      "                                                   'missing_value_strategy': 'fill_with_const'}},\n",
      "                    'image': {   'preprocessing': {   'in_memory': True,\n",
      "                                                      'infer_image_dimensions': True,\n",
      "                                                      'infer_image_max_height': 256,\n",
      "                                                      'infer_image_max_width': 256,\n",
      "                                                      'infer_image_num_channels': True,\n",
      "                                                      'infer_image_sample_size': 100,\n",
      "                                                      'missing_value_strategy': 'backfill',\n",
      "                                                      'num_processes': 1,\n",
      "                                                      'resize_method': 'interpolate',\n",
      "                                                      'scaling': 'pixel_normalization'}},\n",
      "                    'number': {   'preprocessing': {   'fill_value': 0,\n",
      "                                                       'missing_value_strategy': 'fill_with_const',\n",
      "                                                       'normalization': None}},\n",
      "                    'sequence': {   'preprocessing': {   'fill_value': '<UNK>',\n",
      "                                                         'lowercase': False,\n",
      "                                                         'max_sequence_length': 256,\n",
      "                                                         'missing_value_strategy': 'fill_with_const',\n",
      "                                                         'most_common': 20000,\n",
      "                                                         'padding': 'right',\n",
      "                                                         'padding_symbol': '<PAD>',\n",
      "                                                         'tokenizer': 'space',\n",
      "                                                         'unknown_symbol': '<UNK>',\n",
      "                                                         'vocab_file': None}},\n",
      "                    'set': {   'preprocessing': {   'fill_value': '<UNK>',\n",
      "                                                    'lowercase': False,\n",
      "                                                    'missing_value_strategy': 'fill_with_const',\n",
      "                                                    'most_common': 10000,\n",
      "                                                    'tokenizer': 'space'}},\n",
      "                    'text': {   'preprocessing': {   'fill_value': '<UNK>',\n",
      "                                                     'lowercase': True,\n",
      "                                                     'max_sequence_length': 256,\n",
      "                                                     'missing_value_strategy': 'fill_with_const',\n",
      "                                                     'most_common': 20000,\n",
      "                                                     'padding': 'right',\n",
      "                                                     'padding_symbol': '<PAD>',\n",
      "                                                     'pretrained_model_name_or_path': None,\n",
      "                                                     'tokenizer': 'space_punct',\n",
      "                                                     'unknown_symbol': '<UNK>',\n",
      "                                                     'vocab_file': None}},\n",
      "                    'timeseries': {   'preprocessing': {   'fill_value': '',\n",
      "                                                           'missing_value_strategy': 'fill_with_const',\n",
      "                                                           'padding': 'right',\n",
      "                                                           'padding_value': 0,\n",
      "                                                           'timeseries_length_limit': 256,\n",
      "                                                           'tokenizer': 'space'}},\n",
      "                    'vector': {   'preprocessing': {   'fill_value': '',\n",
      "                                                       'missing_value_strategy': 'fill_with_const'}}},\n",
      "    'input_features': [   {   'column': 'Pclass',\n",
      "                              'name': 'Pclass',\n",
      "                              'proc_column': 'Pclass_mZFLky',\n",
      "                              'tied': None,\n",
      "                              'type': 'category'},\n",
      "                          {   'column': 'Sex',\n",
      "                              'name': 'Sex',\n",
      "                              'proc_column': 'Sex_mZFLky',\n",
      "                              'tied': None,\n",
      "                              'type': 'category'},\n",
      "                          {   'column': 'Age',\n",
      "                              'name': 'Age',\n",
      "                              'preprocessing': {   'missing_value_strategy': 'fill_with_mean'},\n",
      "                              'proc_column': 'Age_DF6VxJ',\n",
      "                              'tied': None,\n",
      "                              'type': 'number'},\n",
      "                          {   'column': 'SibSp',\n",
      "                              'name': 'SibSp',\n",
      "                              'proc_column': 'SibSp_mZFLky',\n",
      "                              'tied': None,\n",
      "                              'type': 'number'},\n",
      "                          {   'column': 'Parch',\n",
      "                              'name': 'Parch',\n",
      "                              'proc_column': 'Parch_mZFLky',\n",
      "                              'tied': None,\n",
      "                              'type': 'number'},\n",
      "                          {   'column': 'Fare',\n",
      "                              'name': 'Fare',\n",
      "                              'preprocessing': {   'missing_value_strategy': 'fill_with_mean'},\n",
      "                              'proc_column': 'Fare_DF6VxJ',\n",
      "                              'tied': None,\n",
      "                              'type': 'number'},\n",
      "                          {   'column': 'Embarked',\n",
      "                              'name': 'Embarked',\n",
      "                              'proc_column': 'Embarked_mZFLky',\n",
      "                              'tied': None,\n",
      "                              'type': 'category'}],\n",
      "    'model_type': 'ecd',\n",
      "    'output_features': [   {   'column': 'Survived',\n",
      "                               'decoder': 'generator',\n",
      "                               'dependencies': [],\n",
      "                               'loss': {   'class_similarities_temperature': 0,\n",
      "                                           'class_weights': 1,\n",
      "                                           'confidence_penalty': 0,\n",
      "                                           'robust_lambda': 0,\n",
      "                                           'type': 'sequence_softmax_cross_entropy',\n",
      "                                           'unique': False,\n",
      "                                           'weight': 1},\n",
      "                               'name': 'Survived',\n",
      "                               'preprocessing': {   'missing_value_strategy': 'drop_row'},\n",
      "                               'proc_column': 'Survived_mZFLky',\n",
      "                               'reduce_dependencies': 'sum',\n",
      "                               'reduce_input': 'sum',\n",
      "                               'type': 'text'}],\n",
      "    'preprocessing': {   'oversample_minority': None,\n",
      "                         'sample_ratio': 1.0,\n",
      "                         'split': {'probabilities': [0.7, 0, 0.3]},\n",
      "                         'undersample_majority': None},\n",
      "    'trainer': {   'batch_size': 128,\n",
      "                   'checkpoints_per_epoch': 0,\n",
      "                   'decay': False,\n",
      "                   'decay_rate': 0.96,\n",
      "                   'decay_steps': 10000,\n",
      "                   'early_stop': 3,\n",
      "                   'epochs': 10,\n",
      "                   'eval_batch_size': None,\n",
      "                   'evaluate_training_set': True,\n",
      "                   'gradient_clipping': {   'clipglobalnorm': 0.5,\n",
      "                                            'clipnorm': None,\n",
      "                                            'clipvalue': None},\n",
      "                   'increase_batch_size_eval_metric': 'loss',\n",
      "                   'increase_batch_size_eval_split': 'training',\n",
      "                   'increase_batch_size_on_plateau': 0,\n",
      "                   'increase_batch_size_on_plateau_max': 512,\n",
      "                   'increase_batch_size_on_plateau_patience': 5,\n",
      "                   'increase_batch_size_on_plateau_rate': 2.0,\n",
      "                   'learning_rate': 0.001,\n",
      "                   'learning_rate_scaling': 'linear',\n",
      "                   'learning_rate_warmup_epochs': 1.0,\n",
      "                   'optimizer': {   'amsgrad': False,\n",
      "                                    'betas': (0.9, 0.999),\n",
      "                                    'eps': 1e-08,\n",
      "                                    'lr': 0.001,\n",
      "                                    'type': 'adam',\n",
      "                                    'weight_decay': 0.0},\n",
      "                   'reduce_learning_rate_eval_metric': 'loss',\n",
      "                   'reduce_learning_rate_eval_split': 'training',\n",
      "                   'reduce_learning_rate_on_plateau': 1.0,\n",
      "                   'reduce_learning_rate_on_plateau_patience': 1,\n",
      "                   'reduce_learning_rate_on_plateau_rate': 0.5,\n",
      "                   'regularization_lambda': 0.0,\n",
      "                   'regularization_type': 'l2',\n",
      "                   'should_shuffle': True,\n",
      "                   'staircase': False,\n",
      "                   'steps_per_checkpoint': 0,\n",
      "                   'train_steps': None,\n",
      "                   'type': 'trainer',\n",
      "                   'validation_field': 'combined',\n",
      "                   'validation_metric': 'loss'}}\n",
      "\n",
      "╒═══════════════╕\n",
      "│ PREPROCESSING │\n",
      "╘═══════════════╛\n",
      "\n",
      "Using full dataframe\n",
      "Building dataset (it may take a while)\n",
      "Building dataset: DONE\n",
      "Writing preprocessed training set cache\n",
      "Writing preprocessed test set cache\n",
      "Writing preprocessed validation set cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justinzhao/ludwig/ludwig/utils/torch_utils.py:248: UserWarning: PyTorch has already been initialized. Changes to `gpus`, `gpu_memory_limit`, and `allow_parallel_threads` will be ignored. Start a new Python process to modify these values.\n",
      "  warnings.warn(\n",
      "E0810 20:43:54.489202000 4384081280 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "E0810 20:43:54.507318000 4384081280 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to stack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 216>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m model \u001b[38;5;241m=\u001b[39m LudwigModel(config\u001b[38;5;241m=\u001b[39mconfig, logging_level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# initiate model training\u001b[39;00m\n\u001b[1;32m    212\u001b[0m (\n\u001b[1;32m    213\u001b[0m     train_stats,  \u001b[38;5;66;03m# dictionary containing training statistics\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     preprocessed_data,  \u001b[38;5;66;03m# tuple Ludwig Dataset objects of pre-processed training data\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     output_directory,  \u001b[38;5;66;03m# location of training results stored on disk\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimple_experiment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimple_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# list contents of output directory\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontents of output directory:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_directory)\n",
      "File \u001b[0;32m~/ludwig/ludwig/api.py:444\u001b[0m, in \u001b[0;36mLudwigModel.train\u001b[0;34m(self, dataset, training_set, validation_set, test_set, training_set_metadata, data_format, experiment_name, model_name, model_resume_path, skip_save_training_description, skip_save_training_statistics, skip_save_model, skip_save_progress, skip_save_log, skip_save_processed_input, output_directory, random_seed, **kwargs)\u001b[0m\n\u001b[1;32m    441\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_preprocess_start(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     preprocessed_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_set_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_set_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_resume_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_resume_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_save_training_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_save_training_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_save_training_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_save_training_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_save_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_save_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_save_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_save_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_save_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_save_log\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_save_processed_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_save_processed_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m     (training_set, validation_set, test_set, training_set_metadata) \u001b[38;5;241m=\u001b[39m preprocessed_data\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/ludwig/ludwig/api.py:1312\u001b[0m, in \u001b[0;36mLudwigModel.preprocess\u001b[0;34m(self, dataset, training_set, validation_set, test_set, training_set_metadata, data_format, skip_save_processed_input, random_seed, **kwargs)\u001b[0m\n\u001b[1;32m   1306\u001b[0m print_boxed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPREPROCESSING\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1308\u001b[0m preprocessing_params \u001b[38;5;241m=\u001b[39m merge_config_preprocessing_with_feature_specific_defaults(\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(PREPROCESSING, {}), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(DEFAULTS, {})\n\u001b[1;32m   1310\u001b[0m )\n\u001b[0;32m-> 1312\u001b[0m preprocessed_data \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_for_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_set_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_set_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_save_processed_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_save_processed_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocessing_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocessing_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1327\u001b[0m (proc_training_set, proc_validation_set, proc_test_set, training_set_metadata) \u001b[38;5;241m=\u001b[39m preprocessed_data\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m proc_training_set, proc_validation_set, proc_test_set, training_set_metadata\n",
      "File \u001b[0;32m~/ludwig/ludwig/data/preprocessing.py:1609\u001b[0m, in \u001b[0;36mpreprocess_for_training\u001b[0;34m(config, dataset, training_set, validation_set, test_set, training_set_metadata, data_format, skip_save_processed_input, preprocessing_params, backend, random_seed, callbacks)\u001b[0m\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m use_credentials(backend\u001b[38;5;241m.\u001b[39mcache\u001b[38;5;241m.\u001b[39mcredentials):\n\u001b[1;32m   1608\u001b[0m     logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache processed data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1609\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[43mcache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprocessed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1610\u001b[0m     \u001b[38;5;66;03m# set cached=True to ensure credentials are used correctly below\u001b[39;00m\n\u001b[1;32m   1611\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/ludwig/ludwig/data/cache/manager.py:61\u001b[0m, in \u001b[0;36mDatasetCache.put\u001b[0;34m(self, training_set, test_set, validation_set, training_set_metadata)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_set \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWriting preprocessed validation set cache\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m     validation_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mVALIDATION\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_set_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mVALIDATION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWriting train set metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m data_utils\u001b[38;5;241m.\u001b[39msave_json(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_map[META], training_set_metadata)\n",
      "File \u001b[0;32m~/ludwig/ludwig/data/dataset/pandas.py:94\u001b[0m, in \u001b[0;36mPandasDatasetManager.save\u001b[0;34m(self, cache_path, dataset, config, training_set_metadata, tag)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, cache_path, dataset, config, training_set_metadata, tag):\n\u001b[0;32m---> 94\u001b[0m     \u001b[43msave_hdf5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;241m==\u001b[39m TRAINING:\n\u001b[1;32m     96\u001b[0m         training_set_metadata[DATA_TRAIN_HDF5_FP] \u001b[38;5;241m=\u001b[39m cache_path\n",
      "File \u001b[0;32m~/ludwig/ludwig/utils/data_utils.py:370\u001b[0m, in \u001b[0;36msave_hdf5\u001b[0;34m(data_fp, data)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_hdf5\u001b[39m(data_fp, data):\n\u001b[0;32m--> 370\u001b[0m     numpy_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mto_numpy_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m upload_h5(data_fp) \u001b[38;5;28;01mas\u001b[39;00m h5_file:\n\u001b[1;32m    372\u001b[0m         h5_file\u001b[38;5;241m.\u001b[39mcreate_dataset(HDF5_COLUMNS_KEY, data\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(data\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/ludwig/ludwig/utils/dataframe_utils.py:64\u001b[0m, in \u001b[0;36mto_numpy_dataset\u001b[0;34m(df, backend)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;129;01mand\u001b[39;00m is_dask_backend(backend):\n\u001b[1;32m     63\u001b[0m         res \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[0;32m---> 64\u001b[0m     dataset[col] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/mambaforge/envs/base38/lib/python3.8/site-packages/numpy/core/shape_base.py:422\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    420\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [asanyarray(arr) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m arrays:\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneed at least one array to stack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    424\u001b[0m shapes \u001b[38;5;241m=\u001b[39m {arr\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays}\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shapes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to stack"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# # Simple Model Training Example\n",
    "#\n",
    "# This example is the API example for this Ludwig command line example\n",
    "# (https://ludwig-ai.github.io/ludwig-docs/latest/examples/titanic/).\n",
    "\n",
    "# Import required libraries\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from ludwig.api import LudwigModel\n",
    "from ludwig.datasets import titanic\n",
    "\n",
    "# clean out prior results\n",
    "shutil.rmtree(\"./results\", ignore_errors=True)\n",
    "\n",
    "# Download and prepare the dataset\n",
    "training_set, test_set, _ = titanic.load(split=True)\n",
    "\n",
    "config = {   'combiner': {   'activation': 'relu',\n",
    "                    'bias_initializer': 'zeros',\n",
    "                    'dropout': 0.0,\n",
    "                    'fc_layers': None,\n",
    "                    'flatten_inputs': False,\n",
    "                    'norm': None,\n",
    "                    'norm_params': None,\n",
    "                    'num_fc_layers': 0,\n",
    "                    'output_size': 256,\n",
    "                    'residual': False,\n",
    "                    'type': 'concat',\n",
    "                    'use_bias': True,\n",
    "                    'weights_initializer': 'xavier_uniform'},\n",
    "    'defaults': {   'audio': {   'preprocessing': {   'audio_file_length_limit_in_s': 7.5,\n",
    "                                                      'in_memory': True,\n",
    "                                                      'missing_value_strategy': 'backfill',\n",
    "                                                      'norm': None,\n",
    "                                                      'num_fft_points': None,\n",
    "                                                      'num_filter_bands': 80,\n",
    "                                                      'padding_value': 0,\n",
    "                                                      'type': 'fbank',\n",
    "                                                      'window_length_in_s': 0.04,\n",
    "                                                      'window_shift_in_s': 0.02,\n",
    "                                                      'window_type': 'hamming'}},\n",
    "                    'bag': {   'preprocessing': {   'fill_value': '<UNK>',\n",
    "                                                    'lowercase': False,\n",
    "                                                    'missing_value_strategy': 'fill_with_const',\n",
    "                                                    'most_common': 10000,\n",
    "                                                    'tokenizer': 'space'}},\n",
    "                    'binary': {   'preprocessing': {   'missing_value_strategy': 'fill_with_false'}},\n",
    "                    'category': {   'preprocessing': {   'fill_value': '<UNK>',\n",
    "                                                         'lowercase': False,\n",
    "                                                         'missing_value_strategy': 'fill_with_const',\n",
    "                                                         'most_common': 10000}},\n",
    "                    'date': {   'preprocessing': {   'datetime_format': None,\n",
    "                                                     'fill_value': '',\n",
    "                                                     'missing_value_strategy': 'fill_with_const'}},\n",
    "                    'h3': {   'preprocessing': {   'fill_value': 576495936675512319,\n",
    "                                                   'missing_value_strategy': 'fill_with_const'}},\n",
    "                    'image': {   'preprocessing': {   'in_memory': True,\n",
    "                                                      'infer_image_dimensions': True,\n",
    "                                                      'infer_image_max_height': 256,\n",
    "                                                      'infer_image_max_width': 256,\n",
    "                                                      'infer_image_num_channels': True,\n",
    "                                                      'infer_image_sample_size': 100,\n",
    "                                                      'missing_value_strategy': 'backfill',\n",
    "                                                      'num_processes': 1,\n",
    "                                                      'resize_method': 'interpolate',\n",
    "                                                      'scaling': 'pixel_normalization'}},\n",
    "                    'number': {   'preprocessing': {   'fill_value': 0,\n",
    "                                                       'missing_value_strategy': 'fill_with_const',\n",
    "                                                       'normalization': None}},\n",
    "                    'sequence': {   'preprocessing': {   'fill_value': '<UNK>',\n",
    "                                                         'lowercase': False,\n",
    "                                                         'max_sequence_length': 256,\n",
    "                                                         'missing_value_strategy': 'fill_with_const',\n",
    "                                                         'most_common': 20000,\n",
    "                                                         'padding': 'right',\n",
    "                                                         'padding_symbol': '<PAD>',\n",
    "                                                         'tokenizer': 'space',\n",
    "                                                         'unknown_symbol': '<UNK>',\n",
    "                                                         'vocab_file': None}},\n",
    "                    'set': {   'preprocessing': {   'fill_value': '<UNK>',\n",
    "                                                    'lowercase': False,\n",
    "                                                    'missing_value_strategy': 'fill_with_const',\n",
    "                                                    'most_common': 10000,\n",
    "                                                    'tokenizer': 'space'}},\n",
    "                    'text': {   'preprocessing': {   'fill_value': '<UNK>',\n",
    "                                                     'lowercase': True,\n",
    "                                                     'max_sequence_length': 256,\n",
    "                                                     'missing_value_strategy': 'fill_with_const',\n",
    "                                                     'most_common': 20000,\n",
    "                                                     'padding': 'right',\n",
    "                                                     'padding_symbol': '<PAD>',\n",
    "                                                     'pretrained_model_name_or_path': None,\n",
    "                                                     'tokenizer': 'space_punct',\n",
    "                                                     'unknown_symbol': '<UNK>',\n",
    "                                                     'vocab_file': None}},\n",
    "                    'timeseries': {   'preprocessing': {   'fill_value': '',\n",
    "                                                           'missing_value_strategy': 'fill_with_const',\n",
    "                                                           'padding': 'right',\n",
    "                                                           'padding_value': 0,\n",
    "                                                           'timeseries_length_limit': 256,\n",
    "                                                           'tokenizer': 'space'}},\n",
    "                    'vector': {   'preprocessing': {   'fill_value': '',\n",
    "                                                       'missing_value_strategy': 'fill_with_const'}}},\n",
    "    'input_features': [   {   'column': 'Pclass',\n",
    "                              'name': 'Pclass',\n",
    "                              'proc_column': 'Pclass_mZFLky',\n",
    "                              'tied': None,\n",
    "                              'type': 'category'},\n",
    "                          {   'column': 'Sex',\n",
    "                              'name': 'Sex',\n",
    "                              'proc_column': 'Sex_mZFLky',\n",
    "                              'tied': None,\n",
    "                              'type': 'category'},\n",
    "                          {   'column': 'Age',\n",
    "                              'name': 'Age',\n",
    "                              'preprocessing': {   'missing_value_strategy': 'fill_with_mean'},\n",
    "                              'proc_column': 'Age_DF6VxJ',\n",
    "                              'tied': None,\n",
    "                              'type': 'number'},\n",
    "                          {   'column': 'SibSp',\n",
    "                              'name': 'SibSp',\n",
    "                              'proc_column': 'SibSp_mZFLky',\n",
    "                              'tied': None,\n",
    "                              'type': 'number'},\n",
    "                          {   'column': 'Parch',\n",
    "                              'name': 'Parch',\n",
    "                              'proc_column': 'Parch_mZFLky',\n",
    "                              'tied': None,\n",
    "                              'type': 'number'},\n",
    "                          {   'column': 'Fare',\n",
    "                              'name': 'Fare',\n",
    "                              'preprocessing': {   'missing_value_strategy': 'fill_with_mean'},\n",
    "                              'proc_column': 'Fare_DF6VxJ',\n",
    "                              'tied': None,\n",
    "                              'type': 'number'},\n",
    "                          {   'column': 'Embarked',\n",
    "                              'name': 'Embarked',\n",
    "                              'proc_column': 'Embarked_mZFLky',\n",
    "                              'tied': None,\n",
    "                              'type': 'category'}],\n",
    "    'model_type': 'ecd',\n",
    "    'output_features': [   {   'column': 'Survived',\n",
    "                               'decoder': 'generator',\n",
    "                               'dependencies': [],\n",
    "                               'loss': {   'class_similarities_temperature': 0,\n",
    "                                           'class_weights': 1,\n",
    "                                           'confidence_penalty': 0,\n",
    "                                           'robust_lambda': 0,\n",
    "                                           'type': 'sequence_softmax_cross_entropy',\n",
    "                                           'unique': False,\n",
    "                                           'weight': 1},\n",
    "                               'name': 'Survived',\n",
    "                               'preprocessing': {   'missing_value_strategy': 'drop_row'},\n",
    "                               'proc_column': 'Survived_mZFLky',\n",
    "                               'reduce_dependencies': 'sum',\n",
    "                               'reduce_input': 'sum',\n",
    "                               'type': 'text'}],\n",
    "    'preprocessing': {   'oversample_minority': None,\n",
    "                         'sample_ratio': 1.0,\n",
    "                         'split': {'probabilities': [0.7, 0, 0.3]},\n",
    "                         'undersample_majority': None},\n",
    "    'trainer': {   'batch_size': 128,\n",
    "                   'checkpoints_per_epoch': 0,\n",
    "                   'decay': False,\n",
    "                   'decay_rate': 0.96,\n",
    "                   'decay_steps': 10000,\n",
    "                   'early_stop': 3,\n",
    "                   'epochs': 10,\n",
    "                   'eval_batch_size': None,\n",
    "                   'evaluate_training_set': True,\n",
    "                   'gradient_clipping': {   'clipglobalnorm': 0.5,\n",
    "                                            'clipnorm': None,\n",
    "                                            'clipvalue': None},\n",
    "                   'increase_batch_size_eval_metric': 'loss',\n",
    "                   'increase_batch_size_eval_split': 'training',\n",
    "                   'increase_batch_size_on_plateau': 0,\n",
    "                   'increase_batch_size_on_plateau_max': 512,\n",
    "                   'increase_batch_size_on_plateau_patience': 5,\n",
    "                   'increase_batch_size_on_plateau_rate': 2.0,\n",
    "                   'learning_rate': 0.001,\n",
    "                   'learning_rate_scaling': 'linear',\n",
    "                   'learning_rate_warmup_epochs': 1.0,\n",
    "                   'optimizer': {   'amsgrad': False,\n",
    "                                    'betas': (0.9, 0.999),\n",
    "                                    'eps': 1e-08,\n",
    "                                    'lr': 0.001,\n",
    "                                    'type': 'adam',\n",
    "                                    'weight_decay': 0.0},\n",
    "                   'reduce_learning_rate_eval_metric': 'loss',\n",
    "                   'reduce_learning_rate_eval_split': 'training',\n",
    "                   'reduce_learning_rate_on_plateau': 1.0,\n",
    "                   'reduce_learning_rate_on_plateau_patience': 1,\n",
    "                   'reduce_learning_rate_on_plateau_rate': 0.5,\n",
    "                   'regularization_lambda': 0.0,\n",
    "                   'regularization_type': 'l2',\n",
    "                   'should_shuffle': True,\n",
    "                   'staircase': False,\n",
    "                   'steps_per_checkpoint': 0,\n",
    "                   'train_steps': None,\n",
    "                   'type': 'trainer',\n",
    "                   'validation_field': 'combined',\n",
    "                   'validation_metric': 'loss'}}\n",
    "\n",
    "# Define Ludwig model object that drive model training\n",
    "model = LudwigModel(config=config, logging_level=logging.INFO, backend=\"local\")\n",
    "\n",
    "# initiate model training\n",
    "(\n",
    "    train_stats,  # dictionary containing training statistics\n",
    "    preprocessed_data,  # tuple Ludwig Dataset objects of pre-processed training data\n",
    "    output_directory,  # location of training results stored on disk\n",
    ") = model.train(dataset=training_set, experiment_name=\"simple_experiment\", model_name=\"simple_model\")\n",
    "\n",
    "# list contents of output directory\n",
    "print(\"contents of output directory:\", output_directory)\n",
    "for item in os.listdir(output_directory):\n",
    "    print(\"\\t\", item)\n",
    "\n",
    "# batch prediction\n",
    "model.predict(test_set, skip_save_predictions=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d16d551-7da7-4926-b013-fea66fb8b5c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8892c9b1910420e3bd1222ae54ef3ecb3a2857bdf4a1f10c1bdd49886d4cd072"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
