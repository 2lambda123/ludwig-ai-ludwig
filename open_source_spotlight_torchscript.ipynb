{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28db6b03-0e05-49e7-92c1-28e21c2b9136",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c1b3c3-486d-44d8-90a1-8831ca13bf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ludwig.api import LudwigModel\n",
    "\n",
    "model = LudwigModel.load('results/api_experiment_run_3/model')\n",
    "\n",
    "predictions, _ = model.predict(dataset='rotten_tomatoes_test.csv')\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb81254-50fa-4a80-ac0a-0e1a2bdf5932",
   "metadata": {},
   "source": [
    "# Export the model to Torchscript\n",
    "\n",
    "https://ludwig.ai/latest/user_guide/model_export/#torchscript-export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6842ac92-c061-4cb5-acb3-d5985e680f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "███████████████████████\n",
      "█ █ █ █  ▜█ █ █ █ █   █\n",
      "█ █ █ █ █ █ █ █ █ █ ███\n",
      "█ █   █ █ █ █ █ █ █ ▌ █\n",
      "█ █████ █ █ █ █ █ █ █ █\n",
      "█     █  ▟█     █ █   █\n",
      "███████████████████████\n",
      "ludwig v0.7.dev - Export Torchscript\n",
      "\n",
      "Model path: results/api_experiment_run_11/model\n",
      "Saving model only: False\n",
      "output_path is None, defaulting to model_path\n",
      "Output path: results/api_experiment_run_11/model\n",
      "\n",
      "\n",
      "/usr/local/lib/python3.8/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n",
      "/workspaces/ludwig/ludwig/models/gbm.py:135: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert (\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ludwig export_torchscript -m=results/api_experiment_run_11/model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85d5388-04ed-4749-aa5c-278274b447ee",
   "metadata": {},
   "source": [
    "# Use Torchscript to run model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5bdad76-8229-419e-b066-ae3ad6de521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "preprocessor = torch.jit.load(\"results/api_experiment_run_11/model/inference_preprocessor.pt\")\n",
    "predictor = torch.jit.load(\"results/api_experiment_run_11/model/inference_predictor-cpu.pt\")\n",
    "postprocessor = torch.jit.load(\"results/api_experiment_run_11/model/inference_postprocessor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f2dd483-4743-4d8a-8d69-42a0edd73d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'recommended::predictions': tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True, False]), 'recommended::probabilities': tensor([[0.2415, 0.7585],\n",
      "        [0.3624, 0.6376],\n",
      "        [0.1973, 0.8027],\n",
      "        [0.3849, 0.6151],\n",
      "        [0.2594, 0.7406],\n",
      "        [0.2294, 0.7706],\n",
      "        [0.3130, 0.6870],\n",
      "        [0.2874, 0.7126],\n",
      "        [0.4713, 0.5287],\n",
      "        [0.5144, 0.4856]])}\n"
     ]
    }
   ],
   "source": [
    "from ludwig.utils.inference_utils import to_inference_module_input_from_dataframe\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "\n",
    "test_df = pd.read_csv('rotten_tomatoes_test.csv')\n",
    "raw_data = to_inference_module_input_from_dataframe(test_df, config=model.config)\n",
    "\n",
    "preprocessed_data = preprocessor(raw_data)\n",
    "predictions = predictor(preprocessed_data)\n",
    "postprocessed_data = postprocessor(predictions)\n",
    "\n",
    "print(postprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e604bc7e-cc73-4210-8cb6-c37b676b70a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
